import json
import numpy as np
import os
import pickle
from typing import List, Dict, Any
from pathlib import Path

class JSONSearchService:
    def __init__(self, embedding_model, auto_load: bool = False, data_path: str = "./data/processed/"):
        self.embedding_model = embedding_model
        self.data_path = Path(data_path)
        self.documents = []
        
        # ðŸš€ ìž„ë² ë”© ìºì‹œ ê´€ë ¨
        self.section_embeddings = None  # numpy array of embeddings
        self.sections_data = []  # list of section metadata
        self.embeddings_cached = False  # ðŸ”¥ ì¤‘ìš”: ì´ í”Œëž˜ê·¸ê°€ í•µì‹¬!
        
        if auto_load:
            self.load_all_documents()
    
    def add_document(self, json_data: Dict[str, Any]):
        """ìƒˆ JSON ë¬¸ì„œ ì¶”ê°€ ë° ìž„ë² ë”© ìƒì„±/ë¡œë“œ"""
        if "sections" not in json_data:
            print("âŒ sections í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        self.documents = [json_data]
        vehicle_name = self._extract_vehicle_name_from_data(json_data)
        sections_count = len(json_data.get("sections", []))
        
        print(f"ðŸ“„ {vehicle_name} ë§¤ë‰´ì–¼ ì¶”ê°€: {sections_count}ê°œ ì„¹ì…˜")
        
        # ìºì‹œ íŒŒì¼ í™•ì¸ ë° ë¡œë“œ/ìƒì„±
        self._precompute_embeddings(json_data, vehicle_name)
    
    def _precompute_embeddings(self, json_data: Dict[str, Any], vehicle_name: str):
        """ì„¹ì…˜ë³„ ìž„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ìºì‹±"""
        cache_file = self.data_path / f"{vehicle_name}_embeddings.pkl"
        
        # ðŸ” ê¸°ì¡´ ìºì‹œ í™•ì¸
        if cache_file.exists():
            try:
                print(f"ðŸ’¾ {vehicle_name} ê¸°ì¡´ ìž„ë² ë”© ìºì‹œ ë¡œë“œ ì¤‘...")
                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                    self.section_embeddings = cache_data['embeddings']
                    self.sections_data = cache_data['sections_data']
                    self.embeddings_cached = True  # ðŸ”¥ í”Œëž˜ê·¸ ì„¤ì •!
                print(f"âœ… {vehicle_name} ìºì‹œëœ ìž„ë² ë”© ë¡œë“œ ì™„ë£Œ ({len(self.sections_data)}ê°œ ì„¹ì…˜)")
                return
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±: {e}")
        
        # ðŸš« ìºì‹œê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ (ë°°í¬ í™˜ê²½ì—ì„œëŠ” ìƒì„±í•˜ì§€ ì•ŠìŒ)
        print(f"âŒ {vehicle_name} ìž„ë² ë”© ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {cache_file}")
        print("ðŸ’¡ ë¡œì»¬ì—ì„œ create_embeddings.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ìºì‹œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        return
        
    def search_sections(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """ðŸš€ ìµœì í™”ëœ ê²€ìƒ‰: ìºì‹œëœ ìž„ë² ë”© ì‚¬ìš©"""
        
        # ðŸ” ë””ë²„ê¹… ë¡œê·¸
        print(f"ðŸ” [DEBUG] ê²€ìƒ‰ ì‹œìž‘")
        print(f"ðŸ” [DEBUG] documents ê°œìˆ˜: {len(self.documents)}")
        print(f"ðŸ” [DEBUG] embeddings_cached: {self.embeddings_cached}")
        print(f"ðŸ” [DEBUG] sections_data ê°œìˆ˜: {len(self.sections_data)}")
        print(f"ðŸ” [DEBUG] section_embeddings ì¡´ìž¬: {self.section_embeddings is not None}")
        
        if not self.documents or not self.embeddings_cached:
            print(f"âš ï¸ ë¡œë“œëœ ë¬¸ì„œë‚˜ ìž„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        vehicle_name = self._extract_vehicle_name_from_data(self.documents[0])
        print(f"ðŸ” {vehicle_name} ë§¤ë‰´ì–¼ ê²€ìƒ‰ ì‹œìž‘: '{query}'")
        
        # ðŸš€ ì¿¼ë¦¬ë§Œ ìž„ë² ë”© ê³„ì‚° (ì„¹ì…˜ ìž„ë² ë”©ì€ ìž¬ì‚¬ìš©)
        query_embedding = self.embedding_model.encode_query(query)
        query_norm = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)
        
        # ðŸš€ ë²¡í„°í™”ëœ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = np.dot(query_norm, self.section_embeddings.T)[0]
        
        search_results = []
        
        # ê° ì„¹ì…˜ì— ëŒ€í•´ ì ìˆ˜ ê³„ì‚°
        for i, section_data in enumerate(self.sections_data):
            scores = self._calculate_all_scores_optimized(query, section_data, similarities[i])
            total_score = self._calculate_total_score(scores)
            
            if total_score > 0.05:  # ìž„ê³„ê°’
                search_results.append({
                    "score": total_score,
                    "source": section_data["source"],
                    "section_number": section_data["section_number"],
                    "title": section_data["title"],
                    "page_range": section_data["page_range"],
                    "content": section_data["content"],
                    "keywords": section_data["keywords"],
                    "subsections": section_data["subsections"],
                    "match_details": {
                        "title_score": round(scores["title"], 3),
                        "keyword_score": round(scores["keyword"], 3),
                        "content_score": round(scores["content"], 3),
                        "bonus_score": round(scores["bonus"], 3)
                    }
                })
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        search_results.sort(key=lambda x: x["score"], reverse=True)
        
        print(f"ðŸ“Š {vehicle_name} ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ì„¹ì…˜ (âš¡ ìºì‹œ ì‚¬ìš©)")
        for i, result in enumerate(search_results[:3]):
            print(f"  {i+1}. [{result['score']:.3f}] {result['title']} (íŽ˜ì´ì§€ {result['page_range']})")
        
        return search_results[:k]
    
    def _calculate_all_scores_optimized(self, query: str, section_data: Dict, content_similarity: float) -> Dict[str, float]:
        """ìµœì í™”ëœ ì ìˆ˜ ê³„ì‚°: ì½˜í…ì¸  ìœ ì‚¬ë„ëŠ” ë¯¸ë¦¬ ê³„ì‚°ëœ ê°’ ì‚¬ìš©"""
        return {
            "title": self._calculate_title_score(query, section_data["title"]),
            "keyword": self._calculate_keyword_score(query, section_data["keywords"]),
            "content": float(content_similarity),  # ðŸš€ ë¯¸ë¦¬ ê³„ì‚°ëœ ìœ ì‚¬ë„ ì‚¬ìš©
            "bonus": self._calculate_bonus_score(query, section_data)
        }
    
    def _calculate_total_score(self, scores: Dict[str, float]) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        return (scores["title"] * 0.6) + (scores["keyword"] * 0.15) + \
               (scores["content"] * 0.15) + (scores["bonus"] * 0.1)
    
    def _calculate_title_score(self, query: str, title: str) -> float:
        """ì œëª© ë§¤ì¹­ ì ìˆ˜ (ê°„ë‹¨ ë²„ì „)"""
        if not title:
            return 0
        
        query_words = set(query.lower().split())
        title_words = set(title.lower().split())
        
        # ê¸°ë³¸ ë§¤ì¹­
        matches = len(query_words.intersection(title_words))
        return min(matches / max(len(query_words), 1), 1.0)
    
    def _calculate_keyword_score(self, query: str, keywords: List[str]) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (ê°„ë‹¨ ë²„ì „)"""
        if not keywords:
            return 0
        
        query_lower = query.lower()
        matches = sum(1 for keyword in keywords if keyword.lower() in query_lower)
        return min(matches / 2.0, 1.0)
    
    def _calculate_bonus_score(self, query: str, section: Dict) -> float:
        """ë³´ë„ˆìŠ¤ ì ìˆ˜ (ê°„ë‹¨ ë²„ì „)"""
        content = section.get("content", "").lower()
        query_lower = query.lower()
        
        # ë°©ë²•, ì ˆì°¨ ê´€ë ¨ ë³´ë„ˆìŠ¤
        if any(word in query_lower for word in ["ë°©ë²•", "ì ˆì°¨", "ì–´ë–»ê²Œ"]):
            if any(word in content for word in ["ë°©ë²•", "ì ˆì°¨", "ë‹¨ê³„", "í•˜ì‹­ì‹œì˜¤"]):
                return 0.2
        
        return 0.1
    
    def _extract_vehicle_name_from_data(self, json_data: Dict[str, Any]) -> str:
        """JSON ë°ì´í„°ì—ì„œ ì°¨ëŸ‰ëª… ì¶”ì¶œ"""
        file_name = json_data.get("file_name", "").lower()
        
        if "ê·¸ëžœì €" in file_name or "granjer" in file_name or "grandeur" in file_name:
            return "ê·¸ëžœì €"
        elif "ì‹¼íƒ€íŽ˜" in file_name or "santafe" in file_name:
            return "ì‹¼íƒ€íŽ˜"
        elif "ì˜ë‚˜íƒ€" in file_name or "sonata" in file_name:
            return "ì˜ë‚˜íƒ€"
        elif "ì•„ë°˜ë–¼" in file_name or "avante" in file_name:
            return "ì•„ë°˜ë–¼"
        elif "ì½”ë‚˜" in file_name or "kona" in file_name:
            return "ì½”ë‚˜"
        elif "íˆ¬ì‹¼" in file_name or "tucson" in file_name:
            return "íˆ¬ì‹¼"
        elif "íŽ ë¦¬ì„¸ì´ë“œ" in file_name or "íŒ°ë¦¬ì„¸ì´ë“œ" in file_name or "palisade" in file_name:
            return "íŽ ë¦¬ì„¸ì´ë“œ"
        
        return "unknown"
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            "documents_count": len(self.documents),
            "total_sections": sum(len(doc.get("sections", [])) for doc in self.documents),
            "embeddings_cached": self.embeddings_cached,
            "cached_sections": len(self.sections_data) if self.embeddings_cached else 0
        }