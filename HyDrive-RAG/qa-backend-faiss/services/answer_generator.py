import os
import re
from typing import Dict, Any, List

class AnswerGenerator:
    def __init__(self):
        self.openai_available = bool(os.getenv("OPENAI_API_KEY"))

    async def generate_answer(self, question: str, section_data: Dict[str, Any]) -> str:
        cleaned_content = self._clean_content(section_data['content'])
        question_intent = self._analyze_question_intent(question)

        if self.openai_available:
            raw_answer = await self._generate_openai_answer(question, cleaned_content, question_intent, section_data)
        else:
            keywords = self._extract_question_keywords(question)
            relevant = self._extract_relevant_sentences(cleaned_content, keywords)
            raw_answer = self._fallback_answer(question_intent, relevant, section_data)

        return raw_answer

    async def _generate_openai_answer(self, question: str, cleaned_content: str, question_intent: str, section_data: Dict[str, Any]) -> str:
        prompt = f"""
ë‹¹ì‹ ì€ í˜„ëŒ€ìë™ì°¨ ë§¤ë‰´ì–¼ì„ ì¹œê·¼í•˜ê²Œ ì•ˆë‚´í•˜ëŠ” AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ì§ˆë¬¸: "{question}"

ë§¤ë‰´ì–¼ ë‚´ìš©:
{cleaned_content[:1200]}

---

ë‹µë³€ ì‘ì„± ê°€ì´ë“œ:
â€¢ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ í™œìš©í•´ì„œ êµ¬ì¡°í™”ëœ ë‹µë³€ ì‘ì„±
â€¢ ë‹¨ê³„ë³„ ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš° ë²ˆí˜¸ ëª©ë¡ (1. 2. 3.) ì‚¬ìš©í•˜ê³  ê° ë‹¨ê³„ë¥¼ ì™„ì „í•˜ê²Œ ì„¤ëª…
â€¢ í•„ìš”í•œ ëª¨ë“  ë‹¨ê³„ì™€ ì„¸ë¶€ì‚¬í•­ì„ ë¹ ì§ì—†ì´ í¬í•¨
â€¢ ì¤‘ìš”í•œ ë‚´ìš©ì€ **êµµê²Œ** ê°•ì¡°
â€¢ ì£¼ì˜ì‚¬í•­ì€ âš ï¸ **ì£¼ì˜:** í˜•íƒœë¡œ ëª…í™•íˆ êµ¬ë¶„
â€¢ ì ì ˆí•œ ì´ëª¨í‹°ì½˜(ğŸ”§ ğŸš— âœ… ë“±) í™œìš©
â€¢ ì¹œê·¼í•˜ì§€ë§Œ ì •ë³´ ì „ë‹¬ì´ ëª…í™•í•œ í†¤ ìœ ì§€
â€¢ ê° ë‹¨ê³„ë§ˆë‹¤ êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ì‹¤ìš©ì ì¸ íŒ í¬í•¨
â€¢ ì¤€ë¹„ë¬¼, ê³¼ì •, ì™„ë£Œ í›„ í™•ì¸ì‚¬í•­ê¹Œì§€ ì „ì²´ í”„ë¡œì„¸ìŠ¤ í¬í•¨

ë‹µë³€ì€ 800-1200ì ì •ë„ë¡œ ì¶©ë¶„íˆ ìƒì„¸í•˜ê³  ì™„ì „í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ë”°ë¼í•  ìˆ˜ ìˆì„ ì •ë„ë¡œ êµ¬ì²´ì ì´ê³  ì™„ì „í•œ ê°€ì´ë“œë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ë‹µë³€:
"""

        try:
            from openai import OpenAI
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1200,
                temperature=0.3,
            )
            answer = self._make_answer_friendly(response.choices[0].message.content.strip())
            return self._add_source_info(answer, section_data)

        except Exception as e:
            print(f"âŒ OpenAI í˜¸ì¶œ ì—ëŸ¬: {e}")
            return f"ì•—, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ë‹¤ì‹œ í•œ ë²ˆ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”! ğŸ˜Š"

    def _analyze_question_intent(self, question: str) -> str:
        intent_keywords = {
            "ì ê²€í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?": ["ì ê²€", "í™•ì¸", "ì²´í¬"],
            "êµì²´í•˜ë ¤ê³  í•˜ì‹œë‚˜ìš”?": ["êµì²´", "êµí™˜", "ê°ˆê¸°", "ë°”ê¾¸ê¸°"],
            "ê´€ë¦¬ ë°©ë²•ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?": ["ê´€ë¦¬", "ìœ ì§€", "ë³´ê´€", "ì •ë¹„"],
            "ë¬¸ì œê°€ ìˆìœ¼ì‹ ê°€ìš”?": ["ë¬¸ì œ", "ê³ ì¥", "ì´ìƒ", "ì‘ë™ ì•ˆí•¨", "ì´ìŠˆ"],
            "ì‚¬ìš©ë²•ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?": ["ë°©ë²•", "ì–´ë–»ê²Œ", "ì ˆì°¨", "í•˜ëŠ” ë²•", "ì‚¬ìš©"]
        }

        tokens = set(re.findall(r'[ê°€-í£]{2,}', question))
        for intent, keywords in intent_keywords.items():
            if tokens.intersection(keywords):
                return intent

        question_lower = question.lower()
        for intent, keywords in intent_keywords.items():
            if any(kw in question_lower for kw in keywords):
                return intent

        return "ê¶ê¸ˆí•˜ì‹  ë‚´ìš©"

    def _extract_question_keywords(self, question: str) -> List[str]:
        keyword_mapping = {
            "ì ê²€": ["ì ê²€", "í™•ì¸", "ì²´í¬", "ê´€ë¦¬"],
            "êµì²´": ["êµì²´", "êµí™˜", "ê°ˆê¸°", "ë°”ê¾¸ê¸°"],
            "ë°©ë²•": ["ë°©ë²•", "ì ˆì°¨", "ê³¼ì •", "ì–´ë–»ê²Œ"],
            "ì£¼ì˜": ["ì£¼ì˜", "ê²½ê³ ", "ì•ˆì „", "ìœ„í—˜"],
            "ê´€ë¦¬": ["ê´€ë¦¬", "ìœ ì§€", "ë³´ê´€", "ì •ë¹„"]
        }

        question_tokens = re.findall(r'[ê°€-í£]{2,}', question)
        question_set = set(question_tokens)

        extracted_keywords = []
        for key, synonyms in keyword_mapping.items():
            if question_set.intersection(synonyms):
                extracted_keywords.append(key)

        domain_terms = ["íƒ€ì´ì–´", "ì—”ì§„ì˜¤ì¼", "ë°°í„°ë¦¬", "ë¸Œë ˆì´í¬", "ì—ì–´ì»¨", "ì™€ì´í¼", "ëƒ‰ê°ìˆ˜", "ì „êµ¬", "í“¨ì¦ˆ"]
        domain_hits = question_set.intersection(domain_terms)

        return list(set(extracted_keywords) | domain_hits)

    def _extract_relevant_sentences(self, content: str, keywords: List[str]) -> List[str]:
        sentences = [s.strip() for s in content.split('.') if s.strip()]
        relevant = []

        for sentence in sentences:
            score = sum(1 for kw in keywords if kw in sentence)
            if 10 <= len(sentence) <= 100 and score > 0:
                relevant.append((sentence, score))

        relevant.sort(key=lambda x: x[1], reverse=True)
        return [s for s, _ in relevant[:5]]

    def _fallback_answer(self, intent: str, sentences: List[str], section_data: Dict[str, Any]) -> str:
        if not sentences:
            fallback = "ğŸ” **ê²€ìƒ‰ ê²°ê³¼**\n\nê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•´ë³´ì‹œê±°ë‚˜, ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”."
            return self._add_source_info(fallback, section_data)

        result = "ğŸ” **ë§¤ë‰´ì–¼ ê²€ìƒ‰ ê²°ê³¼**\n\n"
        
        intent_info = {
            "ì ê²€í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?": ("ğŸ”§", "ì ê²€ ë°©ë²•"),
            "êµì²´í•˜ë ¤ê³  í•˜ì‹œë‚˜ìš”?": ("ğŸ”„", "êµì²´ ë°©ë²•"),
            "ê´€ë¦¬ ë°©ë²•ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?": ("ğŸ› ï¸", "ê´€ë¦¬ ë°©ë²•"),
            "ë¬¸ì œê°€ ìˆìœ¼ì‹ ê°€ìš”?": ("âš ï¸", "ë¬¸ì œ í•´ê²°"),
            "ì‚¬ìš©ë²•ì„ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?": ("ğŸ“–", "ì‚¬ìš© ë°©ë²•")
        }
        
        icon, title = intent_info.get(intent, ("ğŸ“", "ê´€ë ¨ ì •ë³´"))
        result += f"{icon} **{title}**\n\n"
        
        if len(sentences) == 1:
            result += f"**ì£¼ìš” ë‚´ìš©:**\n{sentences[0]}\n\n"
        else:
            for i, sentence in enumerate(sentences[:8], 1):
                clean_sentence = sentence.strip()
                if len(clean_sentence) > 200:
                    clean_sentence = clean_sentence[:200]
                    if '.' in clean_sentence:
                        clean_sentence = clean_sentence.rsplit('.', 1)[0] + '.'
                    else:
                        clean_sentence += "..."
                result += f"{i}. {clean_sentence}\n"
        
        warning_sentences = [s for s in sentences if any(keyword in s for keyword in ['ì£¼ì˜', 'ìœ„í—˜', 'ê²½ê³ ', 'ì•ˆì „', 'ê¸ˆì§€'])]
        if warning_sentences:
            result += f"\nâš ï¸ **ì£¼ì˜ì‚¬í•­**\n"
            for warning in warning_sentences[:3]:
                result += f"â€¢ {warning.strip()}\n"
        
        tip_sentences = [s for s in sentences if any(keyword in s for keyword in ['íŒ', 'ê¶Œì¥', 'ì¶”ì²œ', 'íš¨ê³¼ì ', 'ì¢‹ì€'])]
        if tip_sentences:
            result += f"\nğŸ’¡ **ìœ ìš©í•œ íŒ**\n"
            for tip in tip_sentences[:2]:
                result += f"â€¢ {tip.strip()}\n"
        
        result += f"\nğŸ“ **ì¶”ê°€ ë„ì›€**\në” ìì„¸í•œ ë‚´ìš©ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"
        
        return self._add_source_info(result, section_data)

    def _make_answer_friendly(self, text: str) -> str:
        friendly_replacements = {
            r'í•´ì•¼ í•©ë‹ˆë‹¤': 'í•´ì£¼ì„¸ìš”',
            r'í•˜ì‹­ì‹œì˜¤': 'í•´ë³´ì„¸ìš”',
            r'í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤': 'í•˜ì‹œë©´ ë©ë‹ˆë‹¤',
            r'ì£¼ì˜í•˜ì‹­ì‹œì˜¤': 'ì£¼ì˜í•´ì£¼ì„¸ìš”',
            r'í™•ì¸í•˜ì‹­ì‹œì˜¤': 'í™•ì¸í•´ë³´ì„¸ìš”',
            r'ë°˜ë“œì‹œ': 'ê¼­',
            r'í•„ìˆ˜ì ìœ¼ë¡œ': 'ê¼­'
        }
        
        result = text
        for pattern, replacement in friendly_replacements.items():
            result = re.sub(pattern, replacement, result)
        
        # ì£¼ì˜ì‚¬í•­ ê°•ì¡° íŒ¨í„´ - ì´ë¯¸ ê°•ì¡°ëœ ê²ƒì€ ì œì™¸í•˜ê³  ì²˜ë¦¬
        warning_patterns = [
            (r'(?<!âš ï¸ \*\*)(ì£¼ì˜[^.]*\.)', r'âš ï¸ **ì£¼ì˜:** \1'),
            (r'(?<!âš ï¸ \*\*)(ìœ„í—˜[^.]*\.)', r'âš ï¸ **ìœ„í—˜:** \1'),
            (r'(?<!âš ï¸ \*\*)(ê²½ê³ [^.]*\.)', r'âš ï¸ **ê²½ê³ :** \1'),
            (r'(?<!ğŸ›¡ï¸ \*\*)(ì•ˆì „[^.]*\.)', r'ğŸ›¡ï¸ **ì•ˆì „:** \1'),
            (r'(?<!ğŸš« \*\*)(ê¸ˆì§€[^.]*\.)', r'ğŸš« **ê¸ˆì§€:** \1')
        ]
        
        for pattern, replacement in warning_patterns:
            result = re.sub(pattern, replacement, result)
        
        # ì¤‘ë³µëœ ê²½ê³  í‘œì‹œ ì •ë¦¬
        cleanup_patterns = [
            (r'âš ï¸ \*\*ì£¼ì˜:\*\* âš ï¸ \*\*ì£¼ì˜:\*\*', r'âš ï¸ **ì£¼ì˜:**'),
            (r'âš ï¸ \*\*ìœ„í—˜:\*\* âš ï¸ \*\*ìœ„í—˜:\*\*', r'âš ï¸ **ìœ„í—˜:**'),
            (r'âš ï¸ \*\*ê²½ê³ :\*\* âš ï¸ \*\*ê²½ê³ :\*\*', r'âš ï¸ **ê²½ê³ :**'),
            (r'ğŸ›¡ï¸ \*\*ì•ˆì „:\*\* ğŸ›¡ï¸ \*\*ì•ˆì „:\*\*', r'ğŸ›¡ï¸ **ì•ˆì „:**'),
            (r'ğŸš« \*\*ê¸ˆì§€:\*\* ğŸš« \*\*ê¸ˆì§€:\*\*', r'ğŸš« **ê¸ˆì§€:**'),
            # ì¶”ê°€ ì¤‘ë³µ íŒ¨í„´ë“¤
            (r'âš ï¸ \*\*âš ï¸ \*\*ì£¼ì˜: ì£¼ì˜:\*\*', r'âš ï¸ **ì£¼ì˜:**'),
            (r'âš ï¸ \*\*âš ï¸ \*\*ê²½ê³ : ê²½ê³ :\*\*', r'âš ï¸ **ê²½ê³ :**'),
            (r'âš ï¸ \*\*âš ï¸ \*\*ìœ„í—˜: ìœ„í—˜:\*\*', r'âš ï¸ **ìœ„í—˜:**')
        ]
        
        for pattern, replacement in cleanup_patterns:
            result = re.sub(pattern, replacement, result)
        
        important_keywords = [
            r'(ì ì •\s*ê³µê¸°ì••)', r'(ì—”ì§„\s*ì˜¤ì¼)', r'(ë¸Œë ˆì´í¬\s*íŒ¨ë“œ)', 
            r'(ë°°í„°ë¦¬)', r'(íƒ€ì´ì–´)', r'(ëƒ‰ê°ìˆ˜)', r'(í•„í„°)',
            r'(ì ê²€\s*ì£¼ê¸°)', r'(êµì²´\s*ì‹œê¸°)', r'(ì •ê¸°\s*ì ê²€)',
            r'(ì¤€ë¹„ë¬¼)', r'(ë„êµ¬)', r'(ì‘ì—…)', r'(ì ˆì°¨)',
            r'(ë“œë ˆì¸\s*ë³¼íŠ¸)', r'(ì˜¤ì¼\s*íŒ¬)', r'(í† í¬)', r'(ê·œì •ëŸ‰)',
            r'(ì˜¤ì¼\s*ë ˆë²¨)', r'(ë”¥ìŠ¤í‹±)', r'(ì ì„±ë„)', r'(ë“±ê¸‰)'
        ]
        
        for keyword in important_keywords:
            result = re.sub(keyword, r'**\1**', result, flags=re.IGNORECASE)
        
        if len(result) > 1500:
            sentences = result.split('.')
            if len(sentences) > 12:
                result = '. '.join(sentences[:12]) + '.'
        
        return result

    def _add_source_info(self, answer: str, section_data: Dict[str, Any]) -> str:
        # ê¸°ì¡´ ë¬¸êµ¬ ì œê±°
        answer = re.sub(r'\n\nğŸ’¡ ë” ìì„¸í•œ ë‚´ìš©ì€[^\n]*', '', answer)
        answer = re.sub(r'\nğŸ’¡ ë” ìì„¸í•œ ë‚´ìš©ì€[^\n]*', '', answer)
        
        # section_dataì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  í˜ì´ì§€ ê´€ë ¨ í•„ë“œ í™•ì¸
        manual_title = section_data.get('manual_title', section_data.get('title', 'ì‚¬ìš©ì ë§¤ë‰´ì–¼'))
        
        # page_rangeì—ì„œ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
        page_range = section_data.get('page_range', [])
        page = ''
        
        if page_range and isinstance(page_range, list) and len(page_range) > 0:
            start_page = page_range[0]
            if len(page_range) > 1:
                end_page = page_range[1]
                if start_page == end_page:
                    page = str(start_page)  # ë‹¨ì¼ í˜ì´ì§€
                else:
                    page = f"{start_page}-{end_page}"  # í˜ì´ì§€ ë²”ìœ„
            else:
                page = str(start_page)
        
        # ë‹¤ë¥¸ í˜ì´ì§€ í•„ë“œë„ ì‹œë„ (fallback)
        if not page:
            page = (section_data.get('page') or 
                    section_data.get('page_number') or 
                    section_data.get('page_num') or 
                    section_data.get('pg') or 
                    section_data.get('pages') or '')
        
        chapter = (section_data.get('chapter') or 
                  section_data.get('chapter_title') or 
                  section_data.get('section') or 
                  section_data.get('section_number') or '')
        
        section_title = (section_data.get('section_title') or 
                        section_data.get('title') or
                        section_data.get('heading') or 
                        section_data.get('subtitle') or '')
        
        source_info = "\n\n**ì°¸ê³ :** " + str(manual_title)
        
        # í˜ì´ì§€ ì •ë³´ ì²˜ë¦¬
        if page:
            page_str = str(page).strip()
            if page_str and page_str != 'None' and page_str != '':
                # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°
                if page_str.isdigit():
                    source_info += " " + page_str + "í˜ì´ì§€"
                # ë²”ìœ„ì¸ ê²½ìš° (ì˜ˆ: 1-5)
                elif '-' in page_str:
                    source_info += " " + page_str + "í˜ì´ì§€"
                # ì´ë¯¸ 'í˜ì´ì§€'ê°€ í¬í•¨ëœ ê²½ìš°
                elif 'í˜ì´ì§€' in page_str or 'p' in page_str.lower():
                    source_info += " " + page_str
                # ê¸°íƒ€ í˜ì´ì§€ ì •ë³´
                else:
                    source_info += " " + page_str + "í˜ì´ì§€"
        # í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì •ë³´ë¼ë„ í‘œì‹œ
        elif chapter:
            chapter_str = str(chapter).strip()
            if chapter_str and chapter_str != 'None':
                source_info += " (ì„¹ì…˜ " + chapter_str + ")"
        elif section_title and section_title != manual_title:
            section_str = str(section_title).strip()
            if section_str and section_str != 'None':
                source_info += " (" + section_str + ")"
        
        source_info += "\n\nğŸ’¡ ë” ìì„¸í•œ ë‚´ìš©ì€ ê³µì‹ ë§¤ë‰´ì–¼ì—ì„œ í™•ì¸í•˜ì„¸ìš”."
        
        return answer + source_info

    def _clean_content(self, content: str) -> str:
        content = re.sub(r'\*\*([^*]+)\*\*\s*\*\*\1\*\*', r'**\1**', content)
        content = re.sub(r'(\b[ê°€-í£]+)\s+\1', r'\1', content)
        content = re.sub(r'(WL_\w+|ì •ê¸° ì ê²€\s*\d+|2C_\w+)', '', content)

        sentences = content.split('.')
        seen = set()
        unique_sentences = []

        for sentence in sentences:
            sentence = sentence.strip()
            normalized = re.sub(r'\s+', ' ', sentence.lower())
            if normalized and len(normalized) > 10 and normalized not in seen:
                seen.add(normalized)
                unique_sentences.append(sentence)

        return '. '.join(unique_sentences).strip()